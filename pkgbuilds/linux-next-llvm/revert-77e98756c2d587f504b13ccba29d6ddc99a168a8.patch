From 6b39d65c68b0044a1d86eba56e7bb65500b3cb65 Mon Sep 17 00:00:00 2001
From: Nathan Chancellor <nathan@kernel.org>
Date: Tue, 8 Mar 2022 09:58:41 -0700
Subject: [PATCH] Revert "mm: thp: don't have to lock page anymore when
 splitting PMD"

This reverts commit 77e98756c2d587f504b13ccba29d6ddc99a168a8.

This has been dropped from mmotm because it is buggy.

Link: https://lore.kernel.org/r/CAHbLzko0cGBjPzfB28+AKRsb=B_m1NnPoHQ+HdKibP7wH7HxZA@mail.gmail.com/
Signed-off-by: Nathan Chancellor <nathan@kernel.org>
---
 mm/huge_memory.c | 44 +++++++++++++++++++++++++++++++++++++++-----
 1 file changed, 39 insertions(+), 5 deletions(-)

diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 3557aabe86fe..62616253e4b5 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -2133,6 +2133,8 @@ void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
 {
 	spinlock_t *ptl;
 	struct mmu_notifier_range range;
+	bool do_unlock_folio = false;
+	pmd_t _pmd;
 
 	mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, vma->vm_mm,
 				address & HPAGE_PMD_MASK,
@@ -2145,16 +2147,48 @@ void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
 	 * pmd against. Otherwise we can end up replacing wrong folio.
 	 */
 	VM_BUG_ON(freeze && !folio);
-	if (folio && folio != page_folio(pmd_page(*pmd)))
-		goto out;
+	if (folio) {
+		VM_WARN_ON_ONCE(!folio_test_locked(folio));
+		if (folio != page_folio(pmd_page(*pmd)))
+			goto out;
+	}
 
-	if (!(pmd_devmap(*pmd) || is_pmd_migration_entry(*pmd)))
+repeat:
+	if (pmd_trans_huge(*pmd)) {
+		if (!folio) {
+			folio = page_folio(pmd_page(*pmd));
+			/*
+			 * An anonymous page must be locked, to ensure that a
+			 * concurrent reuse_swap_page() sees stable mapcount;
+			 * but reuse_swap_page() is not used on shmem or file,
+			 * and page lock must not be taken when zap_pmd_range()
+			 * calls __split_huge_pmd() while i_mmap_lock is held.
+			 */
+			if (folio_test_anon(folio)) {
+				if (unlikely(!folio_trylock(folio))) {
+					folio_get(folio);
+					_pmd = *pmd;
+					spin_unlock(ptl);
+					folio_lock(folio);
+					spin_lock(ptl);
+					if (unlikely(!pmd_same(*pmd, _pmd))) {
+						folio_unlock(folio);
+						folio_put(folio);
+						folio = NULL;
+						goto repeat;
+					}
+					folio_put(folio);
+				}
+				do_unlock_folio = true;
+			}
+		}
+	} else if (!(pmd_devmap(*pmd) || is_pmd_migration_entry(*pmd)))
 		goto out;
-
 	__split_huge_pmd_locked(vma, pmd, range.start, freeze);
 out:
 	spin_unlock(ptl);
-
+	if (do_unlock_folio)
+		folio_unlock(folio);
 	/*
 	 * No need to double call mmu_notifier->invalidate_range() callback.
 	 * They are 3 cases to consider inside __split_huge_pmd_locked():
-- 
2.35.1

